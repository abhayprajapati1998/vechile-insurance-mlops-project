# vechile-insurance-mlops-project
This `README.md` file describe all the process/method required for a ML end to end.
# ğŸš— Vehicle Data ML Pipeline - End-to-End MLOps Project

Welcome to an enterprise-grade **MLOps Pipeline for Vehicle Data**!  
This repository demonstrates how to build, deploy, and scale a machine learning solution using modern MLOps practices and cloud-native tools.

> âš™ï¸ From **project scaffolding** to **CI/CD deployment on AWS**, this project is designed to give you a complete walkthrough of how production-ready ML systems are built.

---

## ğŸ“Œ Project Highlights

- ğŸ”§ **Project Scaffold Automation** via `template.py`
- ğŸ“¦ **Modular Code Packaging** using `setup.py` & `pyproject.toml`
- ğŸ“Š **MongoDB Integration** for seamless data ingestion
- ğŸ§  **ML Workflow**: EDA â†’ Data Ingestion â†’ Validation â†’ Transformation â†’ Training â†’ Evaluation â†’ Prediction
- â˜ï¸ **AWS Integration**: Model registry in S3, Deployment via EC2 & ECR
- ğŸ” **CI/CD Pipeline** using GitHub Actions & Self-Hosted Runner
- ğŸ³ **Dockerized Deployment** for scalable, cloud-native inference
- ğŸ“ˆ **Live Prediction Web App** on EC2

---

## ğŸš€ Getting Started

### ğŸ§± 1. Project Setup

```bash
# Create template structure
python template.py

# Setup Python virtual environment
conda create -n vehicle python=3.10 -y
conda activate vehicle

# Install dependencies
pip install -r requirements.txt

# Confirm installation
pip list
```

---

### ğŸŒ± 2. Local Package Management

Set up your project for local imports using:
- `setup.py`
- `pyproject.toml`

ğŸ“˜ *More details in `crashcourse.txt`*

---

### ğŸŒ 3. MongoDB Atlas Configuration

1. Create MongoDB Atlas account and cluster (M0 tier)
2. Setup DB user and IP access (`0.0.0.0/0`)
3. Copy the connection string for your project
4. Push local dataset to MongoDB using `mongoDB_demo.ipynb`

> ğŸ’¡ Validate your MongoDB data using the Atlas dashboard (key-value format)

---

### ğŸ›  4. Logging, Exceptions & Notebooks

- âœ… Custom logger integrated and tested
- âš ï¸ Centralized exception handling
- ğŸ“” EDA & Feature Engineering notebooks ready

---

### ğŸ§© 5. Modular ML Pipeline Components

#### ğŸ”¹ Data Ingestion
- Configuration via `constants` and `entity` modules
- Data fetched from MongoDB and converted to DataFrame

#### ğŸ”¹ Data Validation
- Schema-defined validation using `config/schema.yaml`

#### ğŸ”¹ Data Transformation
- Transformers and estimators for scaling and encoding

#### ğŸ”¹ Model Trainer
- Train and persist ML model using structured pipeline

---

### â˜ï¸ 6. AWS Setup for Model Evaluation & Pushing

1. Create IAM user with `AdministratorAccess`
2. Set `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY` via environment variables
3. Create S3 bucket: `my-model-mlopsproj`
4. Implement S3 interaction logic (`aws_connection.py`, `s3_estimator.py`)

> ğŸ“¦ Models are versioned and pushed to S3 after evaluation

---

### ğŸ§ª 7. Model Evaluation & Pusher

- Compare current vs previous model using defined threshold
- Push best model to model registry (S3) for inference

---

### ğŸ§¾ 8. Prediction Pipeline

- REST API served via `Flask` in `app.py`
- Frontend templating with HTML in `template/`
- Static assets in `static/`

---

## ğŸ” CI/CD Workflow

### ğŸ”§ Dockerization

- `Dockerfile` & `.dockerignore` setup
- App containerized for seamless deployment

### ğŸ›  GitHub Actions

- CI/CD pipeline defined in `.github/workflows/aws.yaml`
- Secrets configured in GitHub repo settings

### ğŸš€ AWS ECR & EC2

- Docker images pushed to **AWS ECR**
- Web app hosted on **AWS EC2 (Ubuntu Server 24.04)**

### ğŸ§‘â€ğŸ’» Self-Hosted GitHub Runner

- EC2 configured as GitHub Runner for end-to-end CI/CD
- Port 5000 opened to allow web access

---

## ğŸ”— Final Deployment

- âœ… Push to GitHub â†’ CI/CD triggered â†’ Docker image built â†’ Pushed to ECR
- âœ… EC2 pulls and runs latest image
- ğŸŒ Access the live app via:
  ```
  http://<your-ec2-public-ip>:5000
  ```

- ğŸ‹ï¸ Trigger training manually from:
  ```
  http://<your-ec2-public-ip>:5000/training
  ```

---

## ğŸ“š Folder Structure

```
ğŸ“ src/
â”‚   â”œâ”€â”€ configuration/
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ entity/
â”‚   â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ aws_storage/
â”‚   â””â”€â”€ utils/
ğŸ“ notebook/
ğŸ“ templates/
ğŸ“ static/
ğŸ“„ app.py
ğŸ“„ setup.py
ğŸ“„ requirements.txt
ğŸ“„ pyproject.toml
```

---

## âœ… Key Technologies Used

| Category           | Tools & Services                                      |
|--------------------|-------------------------------------------------------|
| Programming        | Python 3.10                                           |
| Data Storage       | MongoDB Atlas                                         |
| ML Workflow        | pandas, scikit-learn, PyYAML                          |
| Cloud Services     | AWS S3, EC2, ECR, IAM                                 |
| Deployment         | Docker, Flask, GitHub Actions                         |
| CI/CD              | GitHub Actions, Self-Hosted Runner on EC2             |
| Orchestration      | Conda, Environment Variables                          |

---

## ğŸ’¬ Want to Connect?

ğŸ“§ [Email](mailto:abhay.pra.s@gmail.com)  
ğŸ”— [LinkedIn](https://www.linkedin.com/in/abhay-prajapati-2a36b8157/)  
ğŸ“ [Portfolio](https://github.com/abhayprajapati1998)

---

> ğŸ‘¨â€ğŸ’» Built with â¤ï¸ by Abhay Prajapati â€” Designed to make ML projects **production-ready** from Day 1!